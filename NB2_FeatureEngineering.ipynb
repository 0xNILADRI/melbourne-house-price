{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Notebook 2: Feature Engineering\n",
    "\n",
    "We have now acquired a good intuition of our data. Next step involves taking a deep dive into the data. In this section, we will rectify the quality issues with the data.\n",
    "\n",
    "### 2.1. Introduction\n",
    "\n",
    "What do we know of Feature Engineering? The main essense of machine learning ischoosing features, what is a feature and why do we need to engineer it? Simply puting into words all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly.Therefor we require feature engineering. Main objectives :\n",
    "\n",
    "* Prepare the accurate input dataset, compatible with the machine learning algorithm requirements \n",
    "* Improve the performance of machine learning models.\n",
    "\n",
    "### 2.2 Objective\n",
    "\n",
    "Since many features in our data is categorical value, we need to find appropriate techniques to convert them into integers, as our model only inputs numerical values, remove outliers,etc. Few operations to be performed here:\n",
    "1. Imputation\n",
    "2. Handling Outliers\n",
    "3. Extracting Date\n",
    "\n",
    "> Thus, the goal here is to properly mould our dataset to fit perfectly into our model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "\n",
    "# ignore warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load our clean dataset exported from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df_primary = pd.read_csv('data/data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "df_primary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create copy of dataframe\n",
    "df = df_primary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values\n",
    "\n",
    "We need to assess how many null values are present in each column in order to fix the issue. If the values are low we can straight away remove them, if not we can fill them by various measures of centrality (mean, mediam, mode) or by any othe means. But first of all we need to carefully analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullvalper(dataframe):\n",
    "    res = {'col':[],'val':[]}\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].isnull().any() == True:\n",
    "            print(\"{:<15} : {:<6}items, accounts {:.4f}%\".format(\n",
    "                column,dataframe[column].isnull().sum(),dataframe[column].isnull().sum()\n",
    "                /len(dataframe[column])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null percentage\n",
    "nullvalper(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use measurement of centrality (mean, median, mode) to fill the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill using meaurement of centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most suitable way to __fill__ in the __null values__ for __Price, Landsize, Distance, BuildingArea, Lattitude, Longtitude and YearBuilt__ with __median__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price']= df['Price'].fillna(df['Price'].median())\n",
    "df['Landsize']= df['Landsize'].fillna(df['Landsize'].median())\n",
    "df['Distance'] = df['Distance'].fillna(df['Distance'].median())\n",
    "df['BuildingArea']= df['BuildingArea'].fillna(df['BuildingArea'].median())\n",
    "df['Lattitude']= df['Lattitude'].fillna(df['Lattitude'].median())\n",
    "df['Longtitude']= df['Longtitude'].fillna(df['Longtitude'].median())\n",
    "df['YearBuilt']= df['YearBuilt'].fillna(df['YearBuilt'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for __categorical values__ such as __Postcode, Bathroom, Car, CouncilArea, Regionname and Propertycount__ we use __mode__ to __fill__ in the __null values__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bathroom']= df['Bathroom'].fillna(df['Bathroom'].mode()[0])\n",
    "df['Car']= df['Car'].fillna(df['Car'].mode()[0])\n",
    "df['CouncilArea']= df['CouncilArea'].fillna(df['CouncilArea'].mode()[0])\n",
    "df['Regionname']= df['Regionname'].fillna(df['Regionname'].mode()[0])\n",
    "df['Propertycount']= df['Propertycount'].fillna(df['Propertycount'].mode()[0])\n",
    "df['Postcode']= df['Postcode'].fillna(df['Postcode'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "nullvalper(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more null values. We can finally change Data type of Bathroom and Car to Integer, and Date to DateTime (pandas) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Car'] = pd.to_numeric(df['Car']).round(0).astype(int)\n",
    "df['Bathroom'] = pd.to_numeric(df['Bathroom']).round(0).astype(int)\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier detection using Standard Deviation__\n",
    "\n",
    "A distance to the average higher than x * standard deviation can be assumed as an outlier. How should we estimate the value for x. Well there is no hard and fast rule for it, but usually, a value between 2 and 4 seems practical, as estimated by the statisticians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(data,factor):\n",
    "    for column in df.select_dtypes(include=np.number).columns.tolist():\n",
    "        upper_lim = data[column].mean () + data[column].std () * factor\n",
    "        lower_lim = data[column].mean () - data[column].std () * factor\n",
    "        \n",
    "    return data[(data[column] < upper_lim) & (data[column] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier\n",
    "df = remove_outlier(df,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Extra Features\n",
    "\n",
    "We'll try to extract more features out the the current features. For example we can create a column HouseAge for using column Year built - 2018 (dataset creation year), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create extra column for age of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column HouseAge\n",
    "df['HouseAge'] = 2018 - df['YearBuilt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two extra columns, day of year, year, season (all purchase data) using Date column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Day of Year (1-365) and Year of purchase\n",
    "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create Season using the day of the year, as we know the months which have different seasons. We have primarily four seasons namely :\n",
    "1. Spring (March 1 to May 31)\n",
    "2. Summer (June 1 to August 31)\n",
    "3. Autumn (September 1 to November 30)\n",
    "4. Winter (December 1 to February 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide seasons by day of year, using the the above data (month detail)\n",
    "spring = range(60, 152)\n",
    "summer = range(152, 243)\n",
    "autumn = range(243, 334)\n",
    "# everything else would be winter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_series = []\n",
    "\n",
    "for index,i in df.iterrows():\n",
    "    if i['DayOfYear'] in spring:\n",
    "        res = 'spring'\n",
    "    elif i['DayOfYear'] in summer:\n",
    "        res = 'summer'\n",
    "    elif i['DayOfYear'] in autumn:\n",
    "        res = 'autumn'\n",
    "    else:\n",
    "        res = 'winter'\n",
    "    season_series.append(res)\n",
    "    \n",
    "df['season'] = pd.Series(season_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "df.drop(columns=['index'],inplace=True)\n",
    "df.to_csv('data/data_v2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed feature engineering and now we have a clean clean dataset. In the next notebook we proceed with Exploratory Data Analysis to unserstand the data with the help of summary statistics and visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
